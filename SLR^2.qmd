---
title: "Systematic Literature Review for Urbanism"
subtitle: "3. Analysing the corpus"
author:
 - "Bayi Li"
 - "Claudiu Forgaci"
date: "`r Sys.Date()`"
output: html
---

This code block contains all the libraries required in this document. You can load them at once or load them on required.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(pdftools)
library(revtools)
library(tidytext)
library(textstem)
library(topicmodels)
library(ldatuning)
library(tsne)
library(LDAvis)
library(SnowballC)

library(geojsonio)
library(leaflet)
# plot the selected are with sp
library(sp)

library(wordcloud)
library(RColorBrewer)
library(wordcloud2)
library(tm)

library(widyr)
library(furrr)
```

## Load data

**(Optional step)**

This following step is to extract the extracted text files from the PDF files, and sort them into a structured dataframe.

```{r}
ids <-  sub('\\.txt$', '', list.files("data/txts/20240130/")) 

text_files <- list.files("data/txts/20240130/", full.names = TRUE, pattern = ".txt")
texts <- sapply(text_files, read_file)

text_df <- tibble("id" = ids, "text" = texts)
```

In this case, we prepared the two datasets. The first one is **metadata**, which is directly extracted from the paper database; the second one is **full text**, which is extracted from PDF files.

```{r}
#| message: false
metadata <- read_csv(file = "data/metadata.csv")
text_df <- read_csv(file = "data/text_df.csv")
data <- metadata |> left_join(text_df, by = c("id" = "id"))
```

## 0. Basic summary chart

Read the data.

To explore the keywords, we need to extract the three columns "id", "Keywords.Plus", "Year" from the data frame.

```{r data, echo=FALSE, message=FALSE, warning=FALSE}
df <- read.csv("data/final_combined.csv", header = TRUE, sep = ",")

# extract the three columns "id", "Keywords.Plus", "Year"
df_kw <- df[, c("id", "Keywords.Plus", "Year", "Cited.Reference.Count")]

# rename "Keywords.Plus" to "Keywords"
names(df_kw)[names(df_kw) == "Keywords.Plus"] <- "Keywords"
names(df_kw)[names(df_kw) == "Cited.Reference.Count"] <- "Cites"

df_kw
```

Get the number of papers by year.

```{r}
# count the number of papers by year
df_year <- df_kw %>%
  group_by(Year) %>%
  summarise(count = n()) %>%
  arrange(desc(count))
```

Generate a bar chart to show the count of papers by year.

```{r}
# bar chart with y axis only integer
ggplot(df_year, aes(x = Year, y = count)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  scale_y_continuous(breaks = seq(0, 100, 1)) +
  labs(title = "Number of papers by year", x = "Year", y = "Count") +
  theme_minimal()
# save the plot
ggsave("images/number_of_papers_by_year.png")
```

High-citation papers analysis

```{r}
# get the top 10 high-citation papers
df_cites <- df_kw %>%
  arrange(desc(Cites)) %>%
  head(10)
```

Add the number of high-citation papers to the plot of number of papers by year.

```{r}
# add the number of high-citation papers to the plot with dual y axis
ggplot(df_year, aes(x = Year, y = count)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  scale_y_continuous(breaks = seq(0, 100, 1), name = "Number of papers") +
  labs(title = "Number of papers by year", x = "Year") +
  geom_point(data = df_cites, aes(x = Year, y = Cites/10, group = 1), color = "red") +
  scale_y_continuous(sec.axis = sec_axis(~.*10, name = "High-citation papers")) +
  theme_minimal()

# save the plot
ggsave("images/number_of_papers_by_year_high_citations.png")
```

Find the high-citation papers in recent 5 years.

```{r}
# get the high-citation papers in recent 5 years
df_cites_recent <- df_kw %>%
  filter(Year >= 2017) %>%
  arrange(desc(Cites)) %>%
  head(10)
```

Get their article titles.

```{r}
# get the titles of high-citation papers in recent 5 years
df_cites_recent_titles <- df_cites_recent %>%
  left_join(df, by = "id") %>%
  select(id, Article.Title)

# save it as excel
write.csv(df_cites_recent_titles, "output/high_citation_papers_recent_5_years.csv")

```

## 1. Mapping case study locations

Objectives:

1.  Introduce different types of mapping for visualising paper count in the literature database.

2.  Discuss a optimal way to collect data for mapping.

3.  Provide an automatic mapping method (abstract typology of country boundary) for mapping.

Input:\
- manual - extracting location from title - extracting location from abstract - if multiple locations are mentioned, multiple records are created

Output:\
- interactive map with Leaflet - tiled grid map with template available for QGIS as well

Required packages: leaflet, rgdal, sp, rgeos, maptools, ggplot2, ggthemes, ggmap, gridExtra, grid, RColorBrewer, scales, viridis, viridisLite, mapview, tmap, tmaptools, tmapthemes, tmaputi

### 1.1 Preparing the data

The location information normally presented in the text. The computational method is to use the [world cities database](data/world-cities.csv) to conduct a text match from the abstract or full text.

However the reality is there might be spelled in different way. Thus, it is recommended to collect them or, at least, manually check the accuracy of data collection.

Getting the coordinates of the cities could help us easier

```{r libraries, echo=FALSE, message=FALSE, warning=FALSE}
#library(geojsonio)
#install.packages("geojsonsf")
library(geojsonsf)
#library(ggplot2)
#library(leaflet)
# plot the selected are with sp
library(sp)
library(sf)
# interactive map:
library(mapview)
```

```{r}
mapping <- st_read("data/map.csv", options=c("X_POSSIBLE_NAMES=lng","Y_POSSIBLE_NAMES=lat"), crs=4326)

# If you wish to plot the boundaries of countries on the canvas
ggplot() + geom_sf(data = mapping, aes(fill = "black")) + guides(fill = guide_none())
```

1.2 Visulising the data

mapping.1 Create a interactive maps with the package "mapview"

```{r}
mapview(mapping)
```

mapping.2 Static map

Plot the location of studied area on top of the world map (countries' boundary).

Load the world map, which will be the base map for the following mapping.

```{r}
countries_bound <- geojson_sf("data/map_basis/WB_countries_Admin0_10m/WB_countries_Admin0_10m_small.geojson")
countries_bound <- st_make_valid(countries_bound)
 
# If you wish to plot the boundaries of countries on the canvas
ggplot(countries_bound) + geom_sf(aes(fill = FORMAL_EN)) + guides(fill = guide_none())
```

```{r}
ggplot() + geom_sf(data = countries_bound, colour = "white", fill = "black") + geom_sf(data = mapping, colour = "yellow", size = 2) + guides(fill = guide_none())

# Save the plot
# ggsave("direct_mapping.png")
```

mapping.3 Tiled grid map

mapping.3.1 Summarise the number of papaers within different countries

Using the points to join with the countries' boundary

```{r}
country_tiled_frame <- geojson_sf("data/map_basis/Tile-Grid-Map-Cleaned.geojson")

summary = sf::st_join(mapping, countries_bound, join = st_within, left = TRUE)
```

Because the tiled grid map does not contain all the countries in the world, we need to check which countries (values of WB_A3) are not included in the column "alpha-3" of country_tiled_frame.

```{r}
summary$match <- ifelse(summary$WB_A3 %in% country_tiled_frame$"alpha.3", "yeah", "no")
```

```{r}
summary %>%
  filter(match == "no") %>%
  select(c('Geographical.Area', 'FORMAL_EN', 'WB_A3')) %>%
  distinct()
```

We can see that the geographical area "Downtown Jerusalem, Israeli", based on it's coordinates, is categorised as "West Bank and Gaza". However, based on the text of the original paper, regardless of the political status, it should be categorised as "Israel".

In this case, we just recode it to Israel.

Specifically, change the value of WB_A3 from "PSE" to ""ISR".

```{r}
summary$WB_A3[summary$WB_A3 == "PSE"] <- "ISR"
```

recheck if there is unmatch in the data:

```{r}
summary$match <- ifelse(summary$WB_A3 %in% country_tiled_frame$"alpha.3", "yeah", "no")

summary %>%
  filter(match == "no") %>%
  select(c('Geographical.Area', 'FORMAL_EN', 'WB_A3')) %>%
  distinct()
```

Summary the number of papers within different countries:

```{r}
summary_2 <- as.data.frame(summary) %>%
  group_by(WB_A3) %>%
  summarise(n = n())
```

Read the prepared geojson into spatial dataframe:

```{r}
# join the summary_2 to country_tiled_frame based on the column "WB_A3"(left), and "alpha-3"(right)
country_tiled_frame <- left_join(country_tiled_frame, summary_2, by = c("alpha-3" = "WB_A3"))


# fill the NA with 0 in the column "n"
country_tiled_frame$n[is.na(country_tiled_frame$n)] <- 0
```

Save the tiled grid map as a geojson file:

```{r}
writeLines(sf_geojson(country_tiled_frame), "output/tiled_grid_mapped.geojson")

```

Plot the tiled grid map:

```{r map, echo=FALSE, message=FALSE, warning=FALSE}
cvi_colours = list(
  cvi_purples = c("#381532", "#4b1b42", "#5d2252", "#702963",
                 "#833074", "#953784", "#a83e95"),
  cvi_greens = c("#c7eec9", "#86b386", "#749d74", "#637963", "#506b50", "#3f553f", "#2d3f2d"),
  my_favourite_colours = c("#702963", "#637029",    "#296370")
)

cvi_palettes = function(name, n, all_palettes = cvi_colours, type = c("discrete", "continuous")) {
  palette = all_palettes[[name]]
  if (missing(n)) {
    n = length(palette)
  }
  type = match.arg(type)
  out = switch(type,
               continuous = grDevices::colorRampPalette(palette)(n),
               discrete = palette[1:n]
  )
  structure(out, name = name, class = "palette")
}

scale_fill_cvi_c = function(name) {
  ggplot2::scale_fill_gradientn(colours = cvi_palettes(name = name,
                                                     type = "continuous"))
}

```

```{r}
ggplot(country_tiled_frame) +
  geom_sf(aes(fill = n), color = NA) +
  geom_label(data = country_tiled_frame[country_tiled_frame$n != 0,], aes(x= coord_x, y = coord_y, label = `alpha-2`), label.size = NA, fill = NA, color = "grey") +
  scale_fill_cvi_c("cvi_greens") +
  theme_minimal() +
  theme(legend.position = "bottom") +
  labs(title = "Number of papers within different countries") +
  theme_void()

ggsave("images/tiled_grid_map.jpg", width = 10, height = 8, units = "in", dpi = 300)
```

## 2. Keyword analysis (word cloud)

Input: keywords

Output: Word clouds - can be overall or per time period

Required packages: wordcloud, (wordcloud2), RColorBrewer, tm

```{r libraries, echo=FALSE, message=FALSE, warning=FALSE}
library(wordcloud)
library(RColorBrewer)
library(wordcloud2)
library(tm)
```

### 2.1 Create a word cloud for all the keywords:

Make a list of stopwords to remove from the keywords:

```{r}
# remove the rows with NA or empty in the column "Keywords"
df_kw <- df_kw[!is.na(df_kw$Keywords) & df_kw$Keywords != "", ]

# reset the row index
rownames(df_kw) <- NULL

# create a corpus
corpus <- Corpus(VectorSource(df_kw$Keywords))

# convert the corpus to lower case
corpus <- tm_map(corpus, tolower)

# remove punctuation
corpus <- tm_map(corpus, removePunctuation)

# remove numbers
corpus <- tm_map(corpus, removeNumbers)

# remove whitespace
corpus <- tm_map(corpus, stripWhitespace)

stop_words <- c("segregation", "and", "the", "for", "are", "was", "has", "can", "not", "but", "all", "any", "this", "that", "with", "from", "which", "who", "how", "why", "when", "where", "what", "who", "will", "would", "could", "should", "may", "might", "must", "shall", "should", "have", "had", "has", "been", "were", "was", "be", "is", "are", "a", "an", "of", "on", "in", "to", "as", "at", "by", "or", "if", "it", "its", "their", "they", "them", "there", "then", "than", "so", "no", "yes", "up", "down", "out", "into", "over", "under", "between", "through", "about", "after", "before", "between", "under", "over", "above", "below", "through", "across", "against", "along", "around", "behind", "beneath", "beside", "besides", "beyond", "during", "except", "inside", "near", "off", "onto", "outside", "since", "toward", "underneath", "until", "upon", "within", "without", "upon", "mexico", "saopaulo", "rio", "netherlands", "london", "europe", "brazil", "unitedstates")

# remove stopwords (i.e., less than 3 characters)
corpus <- tm_map(corpus, removeWords, stop_words)

# create a document term matrix
dtm <- TermDocumentMatrix(corpus)

# convert the document term matrix to a matrix
m <- as.matrix(dtm)

# calculate the frequency of each word
words <- sort(rowSums(m), decreasing = TRUE)

# create a data frame with two columns: word and frequency
dfkw_feq <- data.frame(word = names(words), freq = words)

# delete intermedia variables
rm(corpus, dtm, m, words)
```

Generating the word cloud for all the years.

```{r}
png("images/wordcloud_total.png", width=6,height=5, units='in', res=300)
set.seed(1234) # for reproducibility 
wordcloud(words = dfkw_feq$word, freq = dfkw_feq$freq, min.freq = 2, max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
# add a title
title(main = "Word Cloud of Keywords")
dev.off()
```

### 2.2 Create a word could for each time period

Divide the time range into 3 periods: before 2000, 2000-2010, and after 2010.

```{r}
time_range <- range(df_kw$Year)

# Divide the time range into 3 periods: before 2000, 2000-2010, and after 2010
periods <- cut(df_kw$Year, breaks = c(time_range[1]-1, 2000, 2010, time_range[2]), labels = c("before 2000", "2000-2010", "after 2010"))

df_kw$periods <- periods
```

Generate the word cloud for each time period.

```{r}
for (i in unique(df_kw$periods)) {
  dfkw_feq <- data.frame()
  corpus <- Corpus(VectorSource(df_kw[df_kw$periods == i, ]$Keywords))
  corpus <- tm_map(corpus, tolower)
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removeWords, stop_words)
  dtm <- TermDocumentMatrix(corpus)
  m <- as.matrix(dtm)
  words <- sort(rowSums(m), decreasing = TRUE)
  dfkw_feq <- data.frame(word = names(words), freq = words)
  png(paste("images/wordcloud_", i, ".png", sep = ""), width=6,height=5, units='in', res=300)
  set.seed(1234) # for reproducibility 
  wordcloud(words = dfkw_feq$word, freq = dfkw_feq$freq, min.freq = 2, max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
  title(main = paste("Word Cloud of Keywords in", i))
  dev.off()
}
```

## 3. Text mining

Input: Full text, machine readable, extracted from PDFs prior to analysis

### 3.1 Word Embeddings

What questions co-occur with a specific word of interest, e.g., segregation?

-   using the LDA model and top words

```{r}
# TODO
# install irlba
install.packages("irlba")
```

-   using word embeddings

Issues to consider: - as a rule of thumb, the dataset should have above 1 mil. tokens for reliable results

```{r}
tidy_data <- data %>% 
  select(id, text) %>% 
  unnest_tokens(word, text) %>% 
  add_count(word) %>% 
  filter(n >= 50) %>% 
  select(-n)

nested_words <- tidy_data %>% 
  nest(words = c(word))

nested_words
```

The following code defines a function to slide a window over the words.

```{r}
slide_windows <- function(tbl, window_size) {
  skipgrams <- slider::slide(
    tbl,
    ~.x,
    .after = window_size - 1,
    .step = 1,
    .complete = TRUE
  )
  
  safe_mutate = safely(mutate)
  
  out <- map2(skipgrams,
              1:length(skipgrams),
              ~ safe_mutate(.x, window_id = .y))
  out %>% 
    transpose() %>% 
    pluck("result") %>% 
    compact() %>% 
    bind_rows()
}
```

The following code calculates the pointwise mutual information (PMI) between words and windows.

```{r}
library(widyr)
library(furrr)

plan(multisession)  ## for parallel processing

tidy_pmi <- nested_words %>% 
  mutate(words = future_map(words, slide_windows, 4L)) %>% 
  unnest(words) %>% 
  unite(window_id, id, window_id) %>% 
  pairwise_pmi(word, window_id)

tidy_pmi
```

The following code calculates the word vectors using the singular value decomposition (SVD) method.

```{r}
tidy_word_vectors <- tidy_pmi %>% 
  widely_svd(
    item1, item2, pmi,
    nv = 100, maxit = 1000
  )

tidy_word_vectors
```

```{r}
nearest_neighbors <- function(df, token) {
  df %>% 
    widely(
      ~ {
        y <- .[rep(token, nrow(.)), ]
        res <- rowSums(. * y) /
          (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[token, ] ^ 2)))
        
        matrix(res, ncol = 1, dimnames = list(x = names(res)))
      },
      sort = TRUE
    )(item1, dimension, value) %>% 
    select(-item2)
}
```

```{r}
tidy_word_vectors %>% 
  nearest_neighbors("income")
```

Output: A bar chart with top words related to the word of interest

```{r}
# Words excluded iteratively while creating the LDA model
# Custom stop words may contain the words used in the literature search query
custom_stop_words <- tibble(word = c("income",
                                     "incomes",
                                     "segregation",
                                     "neighbourhood",
                                     "neighborhood",
                                     "21",
                                     "11"))

tidy_word_vectors %>% 
  nearest_neighbors("income") %>% 
  filter(!item1 %in% custom_stop_words$word) %>% 
  slice_max(value, n = 10) %>% 
  ggplot(aes(reorder(item1, value), value)) +
  geom_col() +
  coord_flip() +
  labs(x = NULL, y = "PMI", title = "Top words related to 'income'")
```

Output: A word cloud with top words related to the word of interest

```{r}
top_10_words <- tidy_word_vectors %>% 
  nearest_neighbors("income") %>% 
  filter(!item1 %in% custom_stop_words$word) %>%
  filter(value > 0) %>%
  mutate(freq = as.integer(value * 1000)) %>%
  slice_max(value, n = 30)
```

```{r}
#png("images/wordcloud_test.png", width=6,height=5, units='in', res=300)
set.seed(1234) # for reproducibility 
wordcloud(words = top_10_words$item1, freq = top_10_words$freq, max.words=400, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
# add a title
title(main = "Word Cloud of Keywords")
#dev.off()
```

### 3.2 Topic model

-   using LDA to identify dominant topics in a corpus of papers

```{r tokenize}
# Words excluded iteratively while creating the LDA model
# Custom stop words may contain the words used in the literature search query
custom_stop_words <- tibble(word = c("income",
                                     "incomes",
                                     "segregation",
                                     "neighbourhood",
                                     "neighborhood"))

words <- data |>
  select(id, text, "year" = `Publication Year`) |>
  unnest_tokens(output = word, input = text) |>  # remove punctuation, convert to lower-case, separate all words
  anti_join(stop_words, by = "word") |>  # remove stop words
  anti_join(custom_stop_words, by = "word") |>  # remove custom stop words
  mutate(word = lemmatize_words(word)) |>  # lemmatise words
  filter(nchar(word) >= 3)  # keep only words that are at least three letters long
```

```{r dtm}
dtm <- words |>
    count(id, word, sort = TRUE) |>
    filter(n > 5) |>  # minimum term frequency 5
    cast_dtm(id, word, n)
```

```{r k}
# Set k empirically
# # This takes around 10 minutes to run and results in a value between 25 and 30
# # which is rather difficult to interpret
# # This might be useful and easy to use if combined with hierarchical representation
# # of topics in a dendrogram - still to be done
# result <- FindTopicsNumber(
#   dtm,
#   topics = seq(from = 2, to = 30, by = 1),
#   metrics = c("CaoJuan2009",  "Deveaud2014"),
#   method = "Gibbs",
#   control = list(seed = 2023),
#   verbose = TRUE)
# k = result$topic[which.max(result$Deveaud2014 - result$CaoJuan2009)]

# Set k qualitatively, based on the researcher;s understanding of the literature
k = 5
```

```{r lda}
# Fit LDA model
lda <- LDA(dtm, k = k, method="Gibbs",
           control = list(seed = 2023, iter = 500, verbose = 100))

# Extract beta and theta statistics from LDA model
beta <- posterior(lda)$terms
theta <- posterior(lda)$topics 

# Add pseudo-names to topics based on the top 5 words in each topic
topic_names <- c()
for (i in 1:nrow(beta)) {
  name <- paste(names(head(sort(beta[i, ], decreasing = TRUE), 5)), collapse = " ")
  topic_names <- c(topic_names, name)
}
```

#### 3.2.1 Topics across the corpus

Output: Bar charts showing top words in the corpus and in each topic

```{r}
terms <- as.data.frame(posterior(lda)$terms)
rownames(terms) <- topic_names

terms <- terms |>
  mutate(topic = rownames(terms)) |>
  pivot_longer(-topic,
               names_to = "term",
               values_to = "prob") |> 
  group_by(term) |>
  mutate(max_topic = topic[which.max(prob)]) |>
  filter(topic == max_topic) |>
  ungroup()

words_topics <- words |>
  left_join(terms, by = c("word" = "term"))

top_terms <- words |>
    group_by(id) |>
    count(word, sort = TRUE) |>
    ungroup() |> 
    slice_max(n, n = 20)

top_terms |>
  left_join(terms, by = c("word" = "term")) |>
  ggplot(aes(reorder(word, n), n)) +
  geom_col(aes(fill = max_topic)) +
  geom_text(aes(label = n), size = 2, hjust = 1.1) +
  coord_flip() +
  xlab("Word") +
  ylab("Frequency") +
  labs(title = paste0("Top ", 20, " most used words in the corpus of theses")) +
  theme_minimal() +
  theme(panel.grid = element_blank())
```

```{r}
# Function to approximate the distance between topics
svd_tsne <- function(x)
  tsne(svd(x)$u)

# Convert DTM into JSON required by the LDAvis package
json <- createJSON(
  phi = beta,
  theta = theta,
  doc.length = rowSums(as.matrix(dtm)),
  vocab = colnames(dtm),
  term.frequency = colSums(as.matrix(dtm)),
  mds.method = svd_tsne,
  plot.opts = list(xlab = "", ylab = "")
)

# Visualise topics model with LDAvis
LDAvis::serVis(json)
```

#### 3.2.2 Evolution of topics

Output: A line chart showing the change in relative importance of topics for a given period.

```{r evolution-topics}
topic_prop_per_year <- theta
colnames(topic_prop_per_year) <- topic_names

viz_df_with_year <-
  pivot_longer(
    data = data.frame(topic_prop_per_year,
                      document = factor(str_sub(
                        rownames(topic_prop_per_year), 1, 20)),
                      check.names = FALSE),
    cols = -document,
    names_to = "topic",
    values_to = "value") |> 
  left_join(mutate(data, id = as.factor(id), "year" = `Publication Year`), by = c("document" = "id"))

# Plot topic proportions per year
viz_df_with_year |> 
  ggplot(aes(x = factor(year), y = value, fill = topic)) + 
  geom_bar(stat = "identity", position = "fill") +
  scale_fill_viridis_d(name = "Topics") +
  xlab("Publication year") +
  ylab("Proportion of topics") +
  labs(title = "Topic proportions per year")
```

```{r evolution-top-terms}
#| warning: false
top_terms <- words |>
  group_by(id) |>
  count(word, sort = TRUE) |>
  ungroup() |>
  slice_max(n, n = 5)

word_order <- words |>
  group_by(word, year) |>
  count(word, sort = TRUE) |>
  ungroup() |>
  arrange(year,-n) |>
  group_by(year) |>
  mutate(relative_frequency = n / sum(n) * 100) |>
  mutate(rank = round(rank(-n), 0)) |>
  ungroup() |>
  filter(word %in% top_terms$word)

ggplot(word_order,
       aes(
         x = year,
         y = relative_frequency,
         color = word,
         label = rank
       )) +
  # aes(fill = "white") +
  geom_smooth(lwd = 0.5) +
  # geom_line() +
  # geom_point(size = 8, shape = 21, fill = "white", stroke = 1.2) +
  # geom_text(ggplot2::aes(x = grad_year), color = "black") +
  ggplot2::scale_color_viridis_d() +
  ggplot2::xlab("Year") + 
  ggplot2::ylab("Frequency (%)") +
  facet_wrap(~ word) +
  theme(
        title = element_blank(),
        axis.text = element_text(size = 12),
        axis.title = element_text(size = 15))
```

<!-- ```{r extracttxt} -->

<!-- files <- list.files(path = "data/collection/PDFs", pattern = "*.pdf", full.names = TRUE) -->

<!-- my_list_output <- list() -->

<!-- for (file in files){ -->

<!--   # Extract text from the PDF -->

<!--   text <- pdftools::pdf_text(file) -->

<!--   # Split each line of text for every new line -->

<!--   lines <- strsplit(text, "\n")[[1]] -->

<!--   # list to store each new line of information in a key value pair -->

<!--   info_list <- lapply(strsplit(lines, ": "), function(x) setNames(x[1], x[2])) -->

<!--   # take the output lines in a single column of a data frame -->

<!--   df <- do.call(rbind, info_list) -->

<!--   # remove rows with NA -->

<!--   df_cleaned <- na.omit(df) -->

<!--   # select 4th row to end of the data frame -->

<!--   df_rows <- data.frame(df_cleaned[4:nrow(df_cleaned), ]) -->

<!--   # rename single column of the dataframe -->

<!--   names(df_rows) = c("Values") -->

<!--   # remove leading and trailing spaces -->

<!--   df_rows$Values <- trimws(df_rows$Values) -->

<!--   # remove leading and trailing spaces -->

<!--   df_rows$Values <- str_replace_all(df_rows$Values, "\\s+", " ") -->

<!--   # select each row which is currently a character string separated by spaces -->

<!--   # split each character from the string and recombine them with "-" separator -->

<!--   # in the process we need to unlist and list the observations of each row -->

<!--   process_row <- function(row) { -->

<!--     mylist = row["Values"] -->

<!--     mylist1 = paste0(unlist(as.list(unlist(strsplit(mylist, '[[:space:]]')))), collapse = "-") -->

<!--     return(mylist1) -->

<!--   } -->

<!--   # apply the above function for all rows in the data-frame -->

<!--   df_rows$ConcatenatedValues <- apply(df_rows, 1, process_row) -->

<!--   # take 1st row value of df_cleaned which looks like unique identifier of the file -->

<!--   id_value = df_cleaned[[1]] -->

<!--   # extract the metric enclosed within parenthesis () -->

<!--   # find position of the string )- after which the metric value is mentioned for each row -->

<!--   # extract the string of metric value bounded by "-" before and after -->

<!--   # replace "," with decimal dot for each metric value -->

<!--   # convert character string to numeric -->

<!--   # transpose the metric as key and corresponding values grouped by the id variable -->

<!--   # remove the second column which looks to be NA -->

<!--   df_transformed = df_rows %>% -->

<!--     extract(ConcatenatedValues, into = c("Metric"), regex = "\\(([^)]+)\\)", remove = FALSE) %>% -->

<!--     mutate(StringPosition = str_locate(ConcatenatedValues, "\\)-")) %>% -->

<!--     mutate(len = nchar(ConcatenatedValues)) %>% -->

<!--     mutate(string = str_sub(ConcatenatedValues, start=StringPosition[,"end"], end=len)) %>% -->

<!--     extract(string, into = c("Metric_Value"), regex = "\\-([^)]+)\\-", remove = FALSE) %>% -->

<!--     select(Values, Metric, Metric_Value) %>% -->

<!--     mutate(Metric_Value = str_replace(Metric_Value, ",", ".")) %>% -->

<!--     mutate(Metric_Value = as.numeric(Metric_Value)) %>% -->

<!--     mutate(id = id_value) %>% -->

<!--     select(id, Metric, Metric_Value) %>% -->

<!--     pivot_wider(names_from = Metric, values_from = Metric_Value) %>% -->

<!--     select(-2) -->

<!--   # store the output of each file in an empty list which can be accessed based on index -->

<!--   my_list_output[[file]] <- df_transformed -->

<!-- } -->

<!-- ## Remove unwanted objects from environment -->

<!-- rm(file, files, id_value, lines, text) -->

<!-- rm(df_rows, df_cleaned, df, info_list, process_row) -->

<!-- my_list_output <- apply(my_list_output,2,as.character) -->

<!-- # save the list as txt -->

<!-- write.table(my_list_output, file = "data/collection/textdf.txt", sep = "\t", row.names = FALSE) -->

<!-- ``` -->

<!-- ```{r extracttxt} -->

<!-- fs::dir_ls("data/collection/PDFs") %>%  -->

<!--   as.character() %>%  -->

<!--   as_tibble() %>%  -->

<!--   mutate(text = map_chr(value, function(x){ -->

<!--     pdftools::pdf_text(x) %>%  -->

<!--       paste0(collapse = "") %>%  -->

<!--       str_remove_all("[\\s\\n\\t\\d[a-z].]") -->

<!--   })) -> textdf -->

<!-- textdf %>%  -->

<!--   write_csv("data/collection/textdf.csv") -->

<!-- textdf -->

<!-- ``` -->

```{python}

import pandas as pd
````
