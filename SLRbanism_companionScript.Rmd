---
title: "SLRbanism_companion"
author: "Cottineau C., Forgaci F., Janssen K., Li B., Zhang S., Zhang X."
date: "2024-03-20"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
Your_scopus_api_key <- readLines(con = "personal_scopus_api_key.md")
```

## Section 2: Search strategy & selection of references
 
### Retrieving references from Scopus


First install libraries for retrieving references from scopus.

```{r}
library(rscopus)
library(RefManageR)
library(tidyverse)
library(brio)
library(glue)
library(revtools)
library(here)
library(remotes)
library(litsearchr)#check the package, maybe it is too old for recent version of r. 
library(igraph)
library(PRISMAstatement)
```
before we start, we need to organize our repository 
```{r}
dir.create("data") #for raw data
dir.create("data_output") #for the unique references 
dir.create("figures")
```




Set your API key, if you do not have, please go to the website of [Elsevier Developer Portal](https://dev.elsevier.com/) to apply, and you will get the key.

In that case, replace `Your_scopus_api_key` by the key value, and add quotes, for instance: options(elsevier_api_key = "my8personal4key")

```{r}
options(elsevier_api_key = Your_scopus_api_key)
```

Set your research query

```{r}
query <- "( ( ( TITLE ( govern* OR state OR decision-making OR policy-making OR stakeholder OR participat* ) ) AND ( TITLE-ABS-KEY ( impact OR outcome OR result OR differentiation OR consequence OR change OR transformation OR role ) ) ) OR ( TITLE-ABS-KEY ( governance W/0 ( mode OR model OR process ) ) ) ) AND 
          ( TITLE-ABS-KEY ( effect OR caus* OR explain* OR influence OR affect OR mechanism OR restrict OR create OR impact OR drive OR role OR transform* OR relation* OR led OR improve OR interven* OR respon* ) ) AND 
          ( TITLE-ABS-KEY ( effect OR caus* OR explain* OR influence OR affect OR mechanism OR restrict OR create OR impact OR drive OR role OR transform* OR relation* OR led OR improve OR interven* OR respon* ) ) AND 
          ( TITLE-ABS-KEY ( urban OR neighborhood OR city OR residential OR regional OR housing ) W/0 ( development OR redevelopment OR regeneration OR restructuring OR revitalization OR construction OR governance ) ) AND 
          ( LIMIT-TO ( DOCTYPE , \"ar\" ) ) AND 
          ( LIMIT-TO ( LANGUAGE , \"English\" ) )"
```

Query scopus if you get the api key. Note that you can modify the max_count for each searching:

```{r}
if (have_api_key()) {
  res <- scopus_search(query = query, max_count = 200, count = 10, view = "COMPLETE")
  search_results <- gen_entries_to_df(res$entries)
}
```

Create an empty list to store search results
```{r}
ids <- search_results$df$pii
search_results_list <- list()
for (id in ids) {
  search_results_list[[id]] <- search_results$df
}

```

Convert the list to a data frame
```{r}
results_df <- do.call(rbind, search_results$df)
transposed_results_df <- t(results_df)
```

What does it look like?
```{r}
head(transposed_results_df)

```

Write the details to a CSV file

```{r}
write.csv(transposed_results_df, here("data", "scopus_api_results.csv"), row.names = FALSE)
```

Export a into a .bib file
```{r}
df <- read.csv(here("data", "scopus_api_results.csv"))
for (i in 1:nrow(df)) {
  df$authkeywords[i] <- paste(unlist(strsplit(df$authkeywords[i], "\\s*\\|\\s*")), collapse = ", ")
}

data <- data.frame(
  Author = df$dc.creator,
  Title = df$dc.title,
  Year = sub(".*\\s(\\d{4})$", "\\1", df$prism.coverDisplayDate),
  Journal = df$prism.publicationName,
  Volume = df$prism.volume,
  Number = df$article.number,
  Pages = df$prism.pageRange,
  DOI = df$prism.doi,
  Keyword = df$authkeywords
)
```

Create a list of BibEntry objects
```{r}
# Format the Keywords field
bib_entries <- lapply(1:nrow(data), function(i) {
  BibEntry(
    bibtype = "Article",        # Add the bibtype argument
    key = paste0(substr(data$Author[i], 1, 1), data$Year[i]),
    author = data$Author[i],    # Add author field
    title = data$Title[i],      # Add title field
    year = data$Year[i],        # Add year field
    journal = data$Journal[i],  # Add journal field
    volume = data$Volume[i],    # Add volume field
    number = data$Number[i],    # Add number field
    pages = data$Pages[i],      # Add pages field
    doi = data$DOI[i],         # Add DOI field
    url = data$Keyword[i]      # Add Keyword field (because BibEntry function does not provide keyword indicator, use url for keywords as an example.)
  )
})
```

Convert each BibEntry object to BibTeX format individually
```{r}
bib_texts <- lapply(bib_entries, toBibtex)
```

Combine the BibTeX texts into a single character vector
```{r}
bib_text <- unlist(bib_texts, use.names = FALSE)
```

Write BibTeX file
```{r}
writeLines(bib_text, here("data","scopus_references.bib"))
```

### Combining tables, deduplicating references and summarising the results



```{r}

wos_data <-read_bibliography(here("data", "wos_reference.bib"))
scopus_data <- read_bibliography(here("data", "scopus_references.bib"))
```

Save variable names of dataframes is object

```{r}
unique_vars_scopus <- colnames(scopus_data)
 unique_vars_wos <- colnames(wos_data)

```


Identify which columns the scopus and wos dataframes have in common
```{r}
common_vars <- intersect(unique_vars_scopus, unique_vars_wos)
print(common_vars)

```

Identify which columns are unique for the scopus and wos references
```{r}
unique_vars_only_scopus <- setdiff(unique_vars_scopus, unique_vars_wos)
print(unique_vars_only_scopus)
unique_vars_only_wos <- setdiff(unique_vars_wos, unique_vars_scopus)
print(unique_vars_only_wos)

```


Select only the variables that appear in both dataframes and the ones you deem relevant. In our case, we select: `label`, `author`, `type`, `title`, `year`, `volume`,  `number`, `doi` 

```{r}
selected_vars <- c("label","author",  "title", "year", "journal", "volume", "number", "doi")

scopus_selection <- scopus_data %>%
  dplyr::select(selected_vars)

wos_selection <- wos_data %>%
  dplyr::select(selected_vars)

```
Now lets check the number of variables (columns) in our dataframes

```{r}
ncol(scopus_selection)

ncol(wos_selection)
```

now lets combine the wos dataframe with the scopus dataframe

```{r}
all_references <- rbind(wos_selection, scopus_selection)

```

##Locate and extract unique references

In order to be able to identify if there are duplicates in our `all_references` dataframe and examine summary statistics of the references-dataset, we need to make sure that variables in the scopus and wos dataframes are represented in a similar way. Sometimes variables differ too much, such that comparison even after alteration becomes difficult (i.e. in our example the variable "author"). However,  for others we we can remove all punctuations and capital letters in order to make the variable structure of the wos refences and scopus refences more similar.

Lets see how we can do that

First create function called `preprocess` which removes all capital letters and punctuations

```{r}
preprocess <- function(text) {
  text <- tolower(text) #all characters are transformed to lower-case
  text <- gsub("[[:punct:]]", "", text) #all punctuations are removed from the characters
  return(text)
}

```

Now lets apply this function on relevant variables in our `all_references` dataframe

```{r}
all_references$journal <- sapply(all_references$journal, preprocess)
all_references$title <- sapply(all_references$title, preprocess)
all_references$label <- sapply(all_references$label, preprocess)
all_references$author <- sapply(all_references$author, preprocess)

```

Examine the scopus_selection dataframe

```{r}
view(all_references)
```

as you can see, all the capital and punctuations are removed from the assigned variables 

```{r}
check_duplicates <- revtools::find_duplicates(all_references, match_variable = "doi")
all_unique_references <- extract_unique_references(all_references, matches = check_duplicates)
```

Compare the total number of references to the total number of unique references

```{r}
#count the number of rows in the all_references dataframe
nrow(all_references)
#do the same for the references_unique dataframe
nrow(all_unique_references)


```

Something else we might want to consider, is if our literature search also identified one or more key articles that we know should be in the SLR. So, lets take a target article. In our case, the author of this specific literature review stated that the articles `Urban governance and social cohesion: Effects of urban restructuring policies in two Dutch cities` by `van Marissing et al. (2006)` and `Rethinking Chinaâ€™s urban governance: The role of the state in neighbourhoods, cities and regions` by `Wu and Zang (2022)` should be considered as a target references. So lets examine, if this article is present in our `all_unique_references` dataframe.  
We do this copying doi from the target references and identifying if this doi is present in the `all_unique_references` dataframe

```{r}
#create a vector with the doi's of van Marissing et al. (2006) and Wu and Zang (2022)
target_references <- c("doi.org/10.1016/j.cities.2005.11.001", "doi.org/10.1177/03091325211062171")


all_unique_references%>%
  filter(doi %in% target_references)

```
the outcome from after filtering the `all_unique_references` dataframe is 0, suggesting that both target references are not prevalent in our `all_unique_references` dataframe. This means that our search query did not identify our target references, suggesting that we need to change our query such that these references are included. Please note, that as we working with a sample of 200 articles from each database, it is most likely that we did not identify the target references due to the 200 reference threshold.  

### Summarise references:
Distribution of publication years
```{r}

all_unique_references$year <- as.numeric(all_unique_references$year)

ggplot(all_unique_references) +
  geom_histogram(aes(x=year), fill = "orange")
```

Top 10 journals publishing on topic:

```{r}
all_unique_references$journal <- as.factor(all_unique_references$journal)
head(summary(all_unique_references$journal),10)

```


Save the remaining references
```{r}

writeLines(all_unique_references, here("data_output","all_unique_references.bib"))


```

## Suggesting new keywords


Import .bib or .ris database
```{r}
refs <- litsearchr::import_results("data")

```

Identify frequent terms
```{r}
raked_terms <- extract_terms(text = refs,
                             method = "fakerake",
                             min_freq=2,
                             min_n=2)
```

Identify frequent keywords tagged by authors
```{r}
keywords <- extract_terms(text = refs,
                          method = "tagged",
                          keywords = refs$keywords,
                          ngrams=T,
                          min_n=2)

```

Create document-feature matrix
```{r}
dfm <- create_dfm(elements = refs$title,
                  features = c(raked_terms, keywords))

```

Create network
```{r}
net <- create_network(search_dfm = dfm,
                      min_studies = 3,
                      min_occ = 3)
hist(igraph::strength(net),
     main="Histogram of node strengths",
     xlab="Node strength")


cutoffs_cumulative <- find_cutoff(net, method = "cumulative")

reduced_graph <- reduce_graph(net, cutoff_strength = cutoffs_cumulative)

plot(reduced_graph)

```


Main keywords
```{r}
search_terms <- litsearchr::get_keywords(reduced_graph)
head(sort(search_terms), 20)
```

Identify isolated components of graph to suggest new keywords
```{r}
components(reduced_graph)
grouped <- split(names(V(reduced_graph)), components(reduced_graph)$membership)
```

Write a new query based on additional keywords
```{r}
litsearchr::write_search(grouped,
                         API_key = NULL,
                         languages = "English",
                         exactphrase = FALSE,
                         stemming = TRUE,
                         directory = "./",
                         writesearch = FALSE,
                         verbose = TRUE,
                         closure = "left")
```



## Drawing the PRISMA figure
```{r}
library(dplyr)

# Assuming 'all_references' and 'previous_results' are your data frames
# containing references and previous results respectively

# Combine 'all_references' with previous results
combined_data <- bind_rows(all_references, previous_results)

# Now 'combined_data' contains both 'all_references' and 'previous_results'
```


```{r}
library(PRISMAstatement)

# Call the prisma() function to generate the PRISMA flowchart
# Replace the values with the actual counts from your study
prisma(
  found = nrow(all_references), # Total number of references found
  found_other = 0,               # Number of additional references found through other sources (if any)
  no_dupes = nrow(all_unique_references), # Number of unique references after removing duplicates
  screened = nrow(all_unique_references),  # Number of references screened
  screen_exclusions = 0,         # Number of references excluded during screening
  full_text = nrow(all_unique_references), # Number of references obtained in full text
  full_text_exclusions = 0,      # Number of references excluded during full-text assessment
  qualitative = nrow(all_unique_references), # Number of studies included in qualitative synthesis
  quantitative = nrow(all_unique_references), # Number of studies included in quantitative synthesis
  width = 800, height = 800      # Specify width and height for the generated PRISMA flowchart
)
```

Ideally, you will stick closely to the PRISMA statement, but small deviations are common. PRISMAstatement gives the option of adding a box which simply calculates the number of duplicates removed.

```{r}

prisma(found = nrow(all_references),
       found_other = 0,
       no_dupes = nrow(all_unique_references), 
       screened = 776, 
       screen_exclusions = 13, 
       full_text = 763,
       full_text_exclusions = 17, 
       qualitative = 746, 
       quantitative = 319,
       extra_dupes_box = TRUE)
```

You can also change the labels, but you will have to insert the number for any label you chang

```{r}
prisma(1000, 20, 270, 270, 10, 260, 20, 240, 107,
       labels = list(found = "FOUND", found_other = "OTHER"))

```

Errors and warnings
```{r}
tryCatch(
  prisma(1, 2, 3, 4, 5, 6, 7, 8, 9),
  error = function(e) e$message)
prisma(1000, 20, 270, 270, 10, 260, 19, 240, 107, 
       width = 100, height = 100)
prisma(1000, 20, 270, 270, 269, 260, 20, 240, 107, 
       width = 100, height = 100)
```

Font_size

```{r}
prisma(1000, 20, 270, 270, 10, 260, 20, 240, 107, font_size = 6)
prisma(1000, 20, 270, 270, 10, 260, 20, 240, 107, font_size = 60)

```


save the Prisma diagram as an object and export it as jpeg 
```{r}
prisma_plot <- prisma(1000, 20, 270, 270, 10, 260, 20, 240, 107, font_size = 60)



# Save as PNG format
png("prisma_flowchart.png", width = 800, height = 800)
# Execute the code to generate PRISMA flowchart (if not already displayed in graphics device)
# prisma_flowchart()
dev.off()

