---
title: "SLRbanism_companion"
author: "Cottineau C., Forgaci F., Janssen K., Li B., Zhang S., Zhang X."
date: "2024-03-20"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
Your_scopus_api_key <- readLines(con = "personal_scopus_api_key.md")
```

## Section 2: Search strategy & selection of references
 
### Retrieving references from Scopus


First install libraries for retrieving references from scopus.

```{r}
library(rscopus)
library(RefManageR)
library(tidyverse)
library(brio)
library(glue)
```

Set your API key, if you do not have, please go to the website of [Elsevier Developer Portal](https://dev.elsevier.com/) to apply, and you will get the key.

In that case, replace `Your_scopus_api_key` by the key value, and add quotes, for instance: options(elsevier_api_key = "my8personal4key")

```{r}
options(elsevier_api_key = Your_scopus_api_key)
```

Set your research query

```{r}
query <- "( ( ( TITLE ( govern* OR state OR decision-making OR policy-making OR stakeholder OR participat* ) ) AND ( TITLE-ABS-KEY ( impact OR outcome OR result OR differentiation OR consequence OR change OR transformation OR role ) ) ) OR ( TITLE-ABS-KEY ( governance W/0 ( mode OR model OR process ) ) ) ) AND 
          ( TITLE-ABS-KEY ( effect OR caus* OR explain* OR influence OR affect OR mechanism OR restrict OR create OR impact OR drive OR role OR transform* OR relation* OR led OR improve OR interven* OR respon* ) ) AND 
          ( TITLE-ABS-KEY ( effect OR caus* OR explain* OR influence OR affect OR mechanism OR restrict OR create OR impact OR drive OR role OR transform* OR relation* OR led OR improve OR interven* OR respon* ) ) AND 
          ( TITLE-ABS-KEY ( urban OR neighborhood OR city OR residential OR regional OR housing ) W/0 ( development OR redevelopment OR regeneration OR restructuring OR revitalization OR construction OR governance ) ) AND 
          ( LIMIT-TO ( DOCTYPE , \"ar\" ) ) AND 
          ( LIMIT-TO ( LANGUAGE , \"English\" ) )"
```

Query scopus if you get the api key. Note that you can modify the max_count for each searching:

```{r}
if (have_api_key()) {
  res <- scopus_search(query = query, max_count = 200, count = 10)
  search_results <- gen_entries_to_df(res$entries)
}
```

Create an empty list to store search results
```{r}
ids <- search_results$df$pii
search_results_list <- list()
for (id in ids) {
  search_results_list[[id]] <- search_results$df
}

```

Convert the list to a data frame
```{r}
results_df <- do.call(rbind, search_results$df)
transposed_results_df <- t(results_df)
```

What does it look like?
```{r}
head(transposed_results_df)
```

Write the details to a CSV file

```{r}
write.csv(transposed_results_df, "scopus_api_results.csv", row.names = FALSE)
```

Export a into a .bib file
```{r}
df <- read.csv("scopus_api_results.csv")
data <- data.frame(
  Author = df$dc.creator,
  Title = df$dc.title,
  Year = sub(".*\\s(\\d{4})$", "\\1", df$prism.coverDisplayDate),
  Journal = df$prism.publicationName,
  Volume = df$prism.volume,
  Number = df$article.number,
  Pages = df$prism.pageRange,
  DOI = df$prism.doi
)
```

Create a list of BibEntry objects
```{r}
bib_entries <- lapply(1:nrow(data), function(i) {
  BibEntry(
    bibtype = "Article",        # Add the bibtype argument
    key = paste0(substr(data$Author[i], 1, 1), data$Year[i]),
    author = data$Author[i],    # Add author field
    title = data$Title[i],      # Add title field
    year = data$Year[i],        # Add year field
    journal = data$Journal[i],  # Add journal field
    volume = data$Volume[i],    # Add volume field
    number = data$Number[i],    # Add number field
    pages = data$Pages[i],      # Add pages field
    doi = data$DOI[i]           # Add DOI field
  )
})
```

Convert each BibEntry object to BibTeX format individually
```{r}
bib_texts <- lapply(bib_entries, toBibtex)
```

Combine the BibTeX texts into a single character vector
```{r}
bib_text <- unlist(bib_texts, use.names = FALSE)
```

Write BibTeX file
```{r}
writeLines(bib_text, "scopus_references.bib")
```

### Combining tables, deduplicating references and summarising the results

```{r}
library(revtools)
library(here)
```

Read a bib or ris file 
```{r}
utf8tolatin1 <- function(infile, outfile) {
  content <- readLines(infile, encoding = "UTF-8")
  latin1 <- iconv(content, from = "UTF-8", to = "us-ascii")
  writeLines(latin1, outfile)
}
utf8tolatin1(here("data", "scopus200.ris"), 
             here("output", "scopus200-latin1.ris"))

# utf8tolatin1 fix taken from https://github.com/mjwestgate/revtools/issues/42
library(synthesisr)
scopus_data <- synthesisr::read_refs(here("output", "scopus200-latin1.ris"))

 scopus_data$journal <- ifelse(scopus_data$source_type == "JOUR",
                               scopus_data$source,NA)
 scopus_data$label <- paste(scopus_data$author, scopus_data$year, scopus_data$title)

wos_data <-read_bibliography(here("output", "lprln.bib"))
```

Save variable names of dataframes is object

```{r}
unique_vars_scopus <- colnames(scopus_data)
 unique_vars_wos <- colnames(wos_data)

```


Identify which columns the scopus and wos dataframes have in common
```{r}
common_vars <- intersect(unique_vars_scopus, unique_vars_wos)
print(common_vars)

```

Identify which columns are unique for the scopus and wos references
```{r}
unique_vars_only_scopus <- setdiff(unique_vars_scopus, unique_vars_wos)
print(unique_vars_only_scopus)
unique_vars_only_wos <- setdiff(unique_vars_wos, unique_vars_scopus)
print(unique_vars_only_wos)

```

Rename the column in the scopus_data from "issue" to "number"
```{r}
colnames(scopus_data)[colnames(scopus_data) == "issue"] <- "number"

```

Select only the variables: `label`, `author`, `type`, `title`, `year`, `volume` and `number` 

```{r}
selected_vars <- c("label", "title", "year", "journal", "volume", "number")

scopus_selection <- scopus_data %>%
  dplyr::select(selected_vars)

head(scopus_selection)

wos_selection <- wos_data %>%
  dplyr::select(selected_vars)

head(wos_selection)

```
To make sure that our variables in the scopus and wos dataframes are represented in a similar way, we can remove all punctuations and capital letters from relevant character variables

First create function called `preprocess` where

```{r}
preprocess <- function(text) {
  text <- tolower(text) #all characters are transformed to lower-case
  text <- gsub("[[:punct:]]", "", text) #all punctuations are removed from the characters
  return(text)
}

```

Apply this function to the scopus_selection dataframe

```{r}
scopus_selection$journal <- sapply(scopus_selection$journal, preprocess)
scopus_selection$title <- sapply(scopus_selection$title, preprocess)
scopus_selection$label <- sapply(scopus_selection$label, preprocess)

```

Examine the scopus_selection dataframe

```{r}
head(scopus_selection)
```

Do the same with wos dataframe

```{r}
wos_selection$journal <- sapply(wos_selection$journal, preprocess)
wos_selection$title <- sapply(wos_selection$title, preprocess)
wos_selection$label <- sapply(wos_selection$label, preprocess)

#examine the wos_selection dataframe
head(wos_selection)

```

Combine the wos dataframe with the scopus dataframe

```{r}
all_references <- rbind(wos_selection, scopus_selection)

```

Locate and extract unique references

```{r}
check_duplicates <- revtools::find_duplicates(all_references)
unique_references <- extract_unique_references(all_references, matches = check_duplicates)
```

Compare the total number of references to the total number of unique references

```{r}
#count the number of rows in the all_references dataframe
nrow(all_references)
#do the same for the references_unique dataframe
nrow(unique_references)

unique_references$year <- as.numeric(unique_references$year)
```

### Summarise references:
Distribution of publication years
```{r}
ggplot(unique_references) +
  geom_histogram(aes(x=year), fill = "orange")
```

Top 10 journals publishing on topic:

```{r}
unique_references$journal <- as.factor(unique_references$journal)
head(summary(unique_references$journal),10)

```


## Suggesting new keywords

```{r}
library(remotes)
#install_github("elizagrames/litsearchr", ref="main")
library(litsearchr)
library(igraph)

```

Import .bib or .ris database
```{r}
refs <- litsearchr::import_results("data")

```

Identify frequent terms
```{r}
raked_terms <- extract_terms(text = refs,
                             method = "fakerake",
                             min_freq=2,
                             min_n=2)
```

Identify frequent keywords tagged by authors
```{r}
keywords <- extract_terms(text = refs,
                          method = "tagged",
                          keywords = refs$keywords,
                          ngrams=T,
                          min_n=2)

```

Create document-feature matrix
```{r}
dfm <- create_dfm(elements = refs$title,
                  features = c(raked_terms, keywords))

```

Create network
```{r}
net <- create_network(search_dfm = dfm,
                      min_studies = 3,
                      min_occ = 3)
hist(igraph::strength(net),
     main="Histogram of node strengths",
     xlab="Node strength")


cutoffs_cumulative <- find_cutoff(net, method = "cumulative")

reduced_graph <- reduce_graph(net, cutoff_strength = cutoffs_cumulative)

plot(reduced_graph)

```


Main keywords
```{r}
search_terms <- litsearchr::get_keywords(reduced_graph)
head(sort(search_terms), 20)
```

Identify isolated components of graph to suggest new keywords
```{r}
components(reduced_graph)
grouped <- split(names(V(reduced_graph)), components(reduced_graph)$membership)
```

Write a new query based on additional keywords
```{r}
litsearchr::write_search(grouped,
                         API_key = NULL,
                         languages = "English",
                         exactphrase = FALSE,
                         stemming = TRUE,
                         directory = "./",
                         writesearch = FALSE,
                         verbose = TRUE,
                         closure = "left")
```



## Drawing the PRISMA figure

```{r}
library(PRISMAstatement)

prisma(found = nrow(all_references),
       found_other = 0,
       no_dupes = nrow(unique_references), 
       screened = nrow(unique_references), 
       screen_exclusions = 0, 
       full_text = nrow(unique_references),
       full_text_exclusions = 0, 
       qualitative = nrow(unique_references), 
       quantitative = nrow(unique_references),
       width = 800, height = 800)
```

Ideally, you will stick closely to the PRISMA statement, but small deviations are common. PRISMAstatement gives the option of adding a box which simply calculates the number of duplicates removed.

```{r}

prisma(found = 750,
       found_other = 123,
       no_dupes = 776, 
       screened = 776, 
       screen_exclusions = 13, 
       full_text = 763,
       full_text_exclusions = 17, 
       qualitative = 746, 
       quantitative = 319,
       extra_dupes_box = TRUE)
```

You can also change the labels, but you will have to insert the number for any label you chang

```{r}
prisma(1000, 20, 270, 270, 10, 260, 20, 240, 107,
       labels = list(found = "FOUND", found_other = "OTHER"))

```

Errors and warnings
```{r}
tryCatch(
  prisma(1, 2, 3, 4, 5, 6, 7, 8, 9),
  error = function(e) e$message)
prisma(1000, 20, 270, 270, 10, 260, 19, 240, 107, 
       width = 100, height = 100)
prisma(1000, 20, 270, 270, 269, 260, 20, 240, 107, 
       width = 100, height = 100)
```

Font_size

```{r}
prisma(1000, 20, 270, 270, 10, 260, 20, 240, 107, font_size = 6)
prisma(1000, 20, 270, 270, 10, 260, 20, 240, 107, font_size = 60)

```
