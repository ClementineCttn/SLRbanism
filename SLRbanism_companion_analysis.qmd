---
title: "Systematic Literature Review for Urbanism"
subtitle: "3. Analysing the corpus"
author:
 - "Bayi Li"
 - "Claudiu Forgaci"
date: "`r Sys.Date()`"
output: html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

## List used packages
packages <- c("tidyverse", "widyr", "furrr", "pdftools", "ggrepel", 
              "revtools", "tidytext", "textstem", "tm", "topicmodels", 
              "ldatuning", "tsne", "LDAvis", "SnowballC", 
              "geojsonio", "geojsonsf", "leaflet", "sp", "sf", "mapview", 
              "wordcloud", "wordcloud2", "RColorBrewer", "reticulate")

## Load packages and install them if needed
for (package in packages) {
  if (!require(package, character.only = TRUE)) install.packages(package)
}
```

## The data

In this notebook we will work with a set of full papers in PDF format and their metadata obtained from Web of Science. The full papers are stored in `data/collection/PDFs/` and the metadata is available in `data/collection/savedrecs_wdoi.csv` of this repository.

If you want to run the code from this notebook on your computer, please follow our [setup instructions](https://github.com/ClementineCttn/SLRbanism?tab=readme-ov-file#setup).

::: note
The same workflow applies to data obtained from other platforms such as Scopus. Note that variable names need to be adjusted accordingly.
:::

## Pre-processing

In this section, we demonstrate a few relevant pre-processing tasks for the data. The pre-processing tasks include: extracting text from PDF files, combining text data with metadata, subsetting and cleaning the data.

### Task 1: Extract text from PDF files {#extract-pdf-text}

Most journal articles are available in PDF format, which is not well suited for quantitative text analysis. Therefore, a common pre-processing task is the extraction of text from PDFs an its storage in a more structured form suitable for analysis. As tools for PDF processing are scarce in the R ecosystem, we rely on Python for this task. The text extraction can be carried out in Python directly or from R via the `reticulate` package. We adopt the latter approach.

The following code chunk will create a Python environment `myenv` and install the Python library `PyMuPDF` for extracting text from PDF files. In subsequent runs, this code chunk will automatically detect the installed environment. The Python script `extract-text-from-pdf.py` is used to extract text from PDF files.

```{r extract-text-from-pdf, message=FALSE, warning=FALSE}
# Set up and use Python for PDF text extraction
virtualenv_create(envname = "myenv", python = install_python())
py_install("PyMuPDF == 1.21.0", envname = "myenv")
use_virtualenv("myenv")
source_python("py/extract-text-from-pdf.py")
```

The following code block will extract text from PDF files and structure them into an R data frame.

```{r}
# Specify the location of the data
data_root <- "data/collection/PDFs"

# Get PDF file paths
pdf_paths <- list.files(data_root, full.names = TRUE, pattern = "*.pdf$")

# Extract text from PDFs with the Python function `convert_pdf()`
text <- convert_pdf(pdf_paths) |> unlist()

# Create data frame with full text data
doc_names <- list.files(data_root, pattern = "*.pdf")
text_df <- tibble(id = str_sub(doc_names, 1, -5L), text) |> 
  mutate(text = str_replace_all(text, "^[0-9]*$", ""))
```

### Task 2: Combine text data with metadata {#metadata}

Next, we combine the **metadata** directly extracted from the paper database with the **full text** extracted from PDF files, using the `id` column to join them.

```{r}
#| message: false
metadata <- read.csv("data/collection/savedrecs_wdoi.csv", header = TRUE, sep = ",")
# take only the last name of first author
metadata$refnames <- sapply(strsplit(metadata$Author.Full.Names, " "), function(x) x[1])
metadata$ref <- paste(metadata$refnames, metadata$Year, sep = "")
# uncomment the line below and load the text data from file if needed
# text_df <- read_csv(file = "data/text_df.csv")
# join the metadata with the text data
data <- metadata |> left_join(text_df, by = c("id" = "id"))
```

### Task 3: Subset and clean the data {#subset-clean-data}

We only need the columns `id`, `Keywords.Plus`, `Year`, `Cited.Reference.Count`, and `ref` from the data frame. We will also rename some columns for better readability.

```{r data, echo=FALSE, message=FALSE, warning=FALSE}
# keep subset of relevant columns
meta_basics <- metadata[, c("id", "Keywords.Plus", "Year", "Cited.Reference.Count", "ref")]

# rename columns
names(meta_basics)[names(meta_basics) == "Keywords.Plus"] <- "Keywords"
names(meta_basics)[names(meta_basics) == "Cited.Reference.Count"] <- "Cites"

head(meta_basics)
```

## Basic exploration of the metadata  

Before looking into we want to gain an overview of the metadata. We will look at the distribution of the number of papers over the years and identify the most cited papers. We do so by asking the following questions:

### Question 1: What is the distribution of number of papers over the years and when were the most cited papers published?  

To answer this question, we plot **the number of papers by year** overlayed with **high-citation papers**. Note the use of two y axes. We can see that the number of papers has been increasing over the years, with most high-citation papers published in recent years. 

```{r}
#| lst-label: lst-papers-per-year
#| lst-cap: Number of papers per year
#| warning: false

# count the number of papers by year
meta_basics_year <- meta_basics %>%
  group_by(Year) %>%
  summarise(count = n()) %>%
  arrange(desc(count))

# get the top 10 high-citation papers
meta_basics_cites <- meta_basics %>%
  arrange(desc(Cites)) %>%
  head(10)

# add the number of high-citation papers to the plot with dual y axis
ggplot(meta_basics_year, aes(x = Year, y = count)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Number of papers by year", x = "Year") +
  geom_text_repel(data = meta_basics_cites, aes(x = Year, y = Cites/10, label = ref), vjust = -0.5, color = "#feb237") +
  geom_point(data = meta_basics_cites, aes(x = Year, y = Cites/10, group = 1), color = "red") +
  scale_y_continuous(breaks = seq(0, 100, 1), name = "Number of papers", sec.axis = sec_axis(~.*10, name = "High-citation papers")) +
  theme_minimal()

# save the plot
# ggsave("output/number_of_papers_by_year_high_citations.png")
```

### Question 2: Which are the top 10 most cited papers between 2017-2023?

We might also want to focus on the most cited papers, assuming that they are the most influential.

```{r}
# get the high-citation papers in recent 5 years
papers_cites_recent <- meta_basics %>%
  filter(Year >= 2017) %>%
  arrange(desc(Cites)) %>%
  head(10)

# get the titles of high-citation papers in recent 5 years
papers_cites_recent_titles <- papers_cites_recent %>%
  left_join(metadata, by = "id") %>%
  select(id, Article.Title)

# save it as excel
write.csv(papers_cites_recent_titles, "output/high_citation_papers_recent_5_years.csv")

papers_cites_recent_titles
```

## Mapping case study locations

Objectives:

1.  Introduce different types of mapping for visualising paper count in the literature database.

2.  Discuss an optimal way to collect data for mapping.

3.  Provide an automatic mapping method (abstract typology of country boundary) for mapping.

Input:

-   manual
-   extracting location from title
-   extracting location from abstract
-   if multiple locations are mentioned, multiple records are created

Output:

-   interactive map with Leaflet
-   tiled grid map with template available for QGIS as well

### Preparing the data  
Location information is normally presented in the text. The computational method is to use the [world cities database](data/world-cities.csv) to conduct a text match from the abstract or full text. However, the reality is there might be spelled in different way. Thus, it is recommended to collect them or, at least, manually check the accuracy of data collection.

```{r}
mapping <- st_read("data/map.csv", options=c("X_POSSIBLE_NAMES=lng","Y_POSSIBLE_NAMES=lat"), crs=4326)
```

### 1.2 Visualising the data

#### 1. Create a interactive maps with the package `mapview`

```{r}
mapview(mapping)
```

#### 2. Static map

Plot the location of studied area on top of the world map (countries' boundary).

Load the world map, which will be the base map for the following mapping.

```{r}
countries_bound <- geojson_sf("data/map_basis/WB_countries_Admin0_10m/WB_countries_Admin0_10m_small.geojson")
 
# If you wish to plot the boundaries of countries on the canvas
ggplot(countries_bound) + 
  geom_sf(aes(fill = FORMAL_EN)) + 
  guides(fill = guide_none())
```

```{r}
ggplot() + 
  geom_sf(data = countries_bound, colour = "white", fill = "black") + 
  geom_sf(data = mapping, colour = "yellow", size = 2) + 
  guides(fill = guide_none())

# Save the plot
ggsave("output/direct_mapping.png")
```

#### 3. Tiled grid map

##### Summarise the number of papers within different countries

Using the points to join with the countries' boundary

```{r}
country_tiled_frame <- geojson_sf("data/map_basis/Tile-Grid-Map-Cleaned.geojson")

countries_bound <- st_make_valid(countries_bound)
summary <- st_join(mapping, countries_bound, join = st_within, left = TRUE)
```

Because the tiled grid map does not contain all the countries in the world, we need to check which countries (values of `WB_A3`) are not included in the column `alpha-3` of country_tiled_frame.

```{r}
summary$match <- ifelse(summary$WB_A3 %in% country_tiled_frame$"alpha-3", "yeah", "no")
```

```{r}
summary %>%
  filter(match == "no") %>%
  select(c('Geographical.Area', 'FORMAL_EN', 'WB_A3')) %>%
  distinct()
```

We can see that the geographical area "Downtown Jerusalem, Israeli", based on it's coordinates, is categorised as "West Bank and Gaza". However, based on the text of the original paper, regardless of the political status, it should be categorised as "Israel".

In this case, we just recode it to Israel. Specifically, we change the value of WB_A3 from `"PSE"` to `"ISR"`.

```{r}
summary$WB_A3[summary$WB_A3 == "PSE"] <- "ISR"
```

We recheck if the match in the data:

```{r}
summary$match <- ifelse(summary$WB_A3 %in% country_tiled_frame$"alpha-3", "yeah", "no")

summary %>%
  filter(match == "no") %>%
  select(c('Geographical.Area', 'FORMAL_EN', 'WB_A3')) %>%
  distinct()
```

Summary the number of papers within different countries:

```{r}
summary_2 <- as.data.frame(summary) %>%
  group_by(WB_A3) %>%
  summarise(n = n())
```

Read the prepared geojson into spatial data frame:

```{r}
# join the summary_2 to country_tiled_frame based on the column `WB_A3` (left), and `alpha-3` (right)
country_tiled_frame <- left_join(country_tiled_frame, summary_2, by = c("alpha-3" = "WB_A3"))

# fill the NA with 0 in the column "n"
country_tiled_frame$n[is.na(country_tiled_frame$n)] <- 0
```

Save the tiled grid map as a geojson file:

```{r}
writeLines(sf_geojson(country_tiled_frame), "output/tiled_grid_mapped.geojson")

```

Plot the tiled grid map:

```{r map, echo=FALSE, message=FALSE, warning=FALSE}
cvi_colours = list(
  cvi_purples = c("#381532", "#4b1b42", "#5d2252", "#702963",
                 "#833074", "#953784", "#a83e95"),
  cvi_greens = c("#c7eec9", "#86b386", "#749d74", "#637963", 
                 "#506b50", "#3f553f", "#2d3f2d"),
  my_favourite_colours = c("#702963", "#637029",    "#296370")
)

cvi_palettes = function(name, n, all_palettes = cvi_colours, type = c("discrete", "continuous")) {
  palette = all_palettes[[name]]
  if (missing(n)) {
    n = length(palette)
  }
  type = match.arg(type)
  out = switch(type,
               continuous = grDevices::colorRampPalette(palette)(n),
               discrete = palette[1:n]
  )
  structure(out, name = name, class = "palette")
}

scale_fill_cvi_c = function(name) {
  ggplot2::scale_fill_gradientn(colours = cvi_palettes(name = name,
                                                     type = "continuous"))
}

```

```{r}
ggplot(country_tiled_frame) +
  geom_sf(aes(fill = n), color = NA) +
  geom_label(data = country_tiled_frame[country_tiled_frame$n != 0,], aes(x= coord_x, y = coord_y, label = `alpha-2`), label.size = NA, fill = NA, color = "grey") +
  scale_fill_cvi_c("cvi_greens") +
  theme_minimal() +
  theme(legend.position = "bottom") +
  labs(title = "Number of papers within different countries") +
  theme_void()

ggsave("output/tiled_grid_map.jpg", width = 10, height = 8, units = "in", dpi = 300)
```

## Keyword analysis (wordcloud)

Looking at the keywords is a quick way to understand the main topics of the literature. We will create a word cloud for all the keywords and for each time period.

### Question 1: What are the main topics in the literature as seen in the keywords?

We will create a word cloud to visualise the most frequent keywords in the literature. We start by removing rows with missing keyword values, as well as stopwords, numbers, extra white spaces and punctuation from the keywords. We then create a document term matrix and calculate the frequency of each word.

```{r}
# remove the rows with NA or empty in the column "Keywords"
df_kw <- data[!is.na(data$Keywords) & data$Keywords != "", ]

# reset the row index
rownames(df_kw) <- NULL

# create a corpus
corpus <- Corpus(VectorSource(df_kw$Keywords))

# convert the corpus to lower case
corpus <- tm_map(corpus, tolower)

# remove punctuation
corpus <- tm_map(corpus, removePunctuation)

# remove numbers
corpus <- tm_map(corpus, removeNumbers)

# remove whitespace
corpus <- tm_map(corpus, stripWhitespace)

stop_words <- c("segregation", "and", "the", "for", "are", "was", "has", "can", "not", "but", "all", "any", "this", "that", "with", "from", "which", "who", "how", "why", "when", "where", "what", "who", "will", "would", "could", "should", "may", "might", "must", "shall", "should", "have", "had", "has", "been", "were", "was", "be", "is", "are", "a", "an", "of", "on", "in", "to", "as", "at", "by", "or", "if", "it", "its", "their", "they", "them", "there", "then", "than", "so", "no", "yes", "up", "down", "out", "into", "over", "under", "between", "through", "about", "after", "before", "between", "under", "over", "above", "below", "through", "across", "against", "along", "around", "behind", "beneath", "beside", "besides", "beyond", "during", "except", "inside", "near", "off", "onto", "outside", "since", "toward", "underneath", "until", "upon", "within", "without", "upon", "mexico", "saopaulo", "rio", "netherlands", "london", "europe", "brazil", "unitedstates")

# remove stopwords (i.e., less than 3 characters)
corpus <- tm_map(corpus, removeWords, stop_words)

# create a document term matrix
dtm <- TermDocumentMatrix(corpus)

# convert the document term matrix to a matrix
m <- as.matrix(dtm)

# calculate the frequency of each word
words <- sort(rowSums(m), decreasing = TRUE)

# create a data frame with two columns: word and frequency
dfkw_feq <- data.frame(word = names(words), freq = words)

# delete intermedia variables
# rm(corpus, dtm, m, words)
```

Using the keyword frequencies, we can create a word cloud to visualise the most frequent keywords in the literature.

```{r}
png("output/wordcloud_total.png", width=6,height=5, units='in', res=300)
set.seed(1234) # for reproducibility 
wordcloud(words = dfkw_feq$word, freq = dfkw_feq$freq, min.freq = 2, max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
# add a title
title(main = "Word Cloud of Keywords")
dev.off()
```

### Question 2: What are the main topics in the literature in different time periods as seen in the keywords?

Looking at different time periods can help us understand the evolution of the literature. We will divide the time range into three periods: before 2000, 2000-2010, and after 2010.

```{r}
time_range <- range(df_kw$Publication.Year)

# Divide the time range into 3 periods: before 2000, 2000-2010, and after 2010
periods <- cut(df_kw$Year, breaks = c(time_range[1]-1, 2000, 2010, time_range[2]), labels = c("before 2000", "2000-2010", "after 2010"))

df_kw$periods <- periods
```

Generate the word cloud for each time period.

```{r eval = FALSE}
for (i in unique(df_kw$periods)) {
  dfkw_feq <- data.frame()
  corpus <- Corpus(VectorSource(df_kw[df_kw$periods == i, ]$Keywords))
  corpus <- tm_map(corpus, tolower)
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removeWords, stop_words)
  dtm <- TermDocumentMatrix(corpus)
  m <- as.matrix(dtm)
  words <- sort(rowSums(m), decreasing = TRUE)
  dfkw_feq <- data.frame(word = names(words), freq = words)
  png(paste("output/wordcloud_", i, ".png", sep = ""), width=6, height=5, units='in', res=300)
  set.seed(1234) # for reproducibility 
  wordcloud(words = dfkw_feq$word, freq = dfkw_feq$freq, min.freq = 2, max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
  title(main = paste("Word Cloud of Keywords in", i))
  dev.off()
}
```

## Text mining

Text mining is a powerful tool for extracting insights from unstructured text data. In this section, we will demonstrate how to perform text mining on the full text of the papers extracted from PDFs. We start by splitting the text into words, a process called tokenisation, and then proceed with word context, word embeddings and topic modelling.

```{r tokens, warning=FALSE}
words <- unnest_tokens(text_df, word, text) |>
  filter(is.na(readr::parse_number(word))) |>     # Remove numbers
  anti_join(tidytext::stop_words, by = "word") |>           # Remove English stop words
  # anti_join(custom_stop_words, by = "word") |>  # Remove custom stop words
  dplyr::filter(nchar(word) > 3)                  # remove words of max. 3 characters
```

```{r tokens-rank}
words_count <- words |> 
  count(word) |> 
  arrange(desc(n))

words_count
```

### Words in context: sentences around a specific word

We can extract sentences around a specific word of interest to understand the context in which it is used.

```{r}
# Define the word of interest
word_of_interest <- "income"

# Extract sentences around the word of interest
sentences <- text_df |>
  unnest_tokens(sentence, text, token = "sentences") |>
  filter(str_detect(sentence, word_of_interest)) |>
  select(sentence)

# Display the first few sentences
head(pull(sentences), 10)
```
Note the high occurrence of the hyphenated "low-income" in the sentences. This is a common issue in text mining, where hyphenated words are treated as separate words. We can address this in two ways: by replacing hyphens with underscores to treat them as a single word (e.g., "low_income") or by re-running the analysis with "bigrams" as tokens instead of "words".

### Words in context: word embeddings

Word embeddings are dense vector representations of words that capture semantic relationships between words. We use word embeddings to find words that are similar to a word of interest such as "income"? As a rule of thumb, the dataset should have above 1 million tokens (words in this case) for reliable results. The following code chunks demonstrate the steps to calculate word embeddings.

```{r}
tidy_data <- data %>% 
  select(id, text) %>% 
  unnest_tokens(word, text) %>% 
  add_count(word) %>% 
  filter(n >= 50) %>% 
  select(-n)

nested_words <- tidy_data %>% 
  nest(words = c(word))

nested_words
```

The following code defines a function to slide a window over the words.

```{r}
slide_windows <- function(tbl, window_size) {
  skipgrams <- slider::slide(
    tbl,
    ~.x,
    .after = window_size - 1,
    .step = 1,
    .complete = TRUE
  )
  
  safe_mutate = safely(mutate)
  
  out <- map2(skipgrams,
              1:length(skipgrams),
              ~ safe_mutate(.x, window_id = .y))
  out %>% 
    transpose() %>% 
    pluck("result") %>% 
    compact() %>% 
    bind_rows()
}
```

The following code calculates the pointwise mutual information (PMI) between words and windows.

```{r}
plan(multisession)  # for parallel processing

tidy_pmi <- nested_words %>% 
  mutate(words = future_map(words, slide_windows, 4L)) %>% 
  unnest(words) %>% 
  unite(window_id, id, window_id) %>% 
  pairwise_pmi(word, window_id)

tidy_pmi
```

The following code calculates the word vectors using the singular value decomposition (SVD) method.

```{r}
tidy_word_vectors <- tidy_pmi %>% 
  widely_svd(
    item1, item2, pmi,
    nv = 100, maxit = 1000
  )

tidy_word_vectors
```

```{r}
nearest_neighbors <- function(df, token) {
  df %>% 
    widely(
      ~ {
        y <- .[rep(token, nrow(.)), ]
        res <- rowSums(. * y) /
          (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[token, ] ^ 2)))
        
        matrix(res, ncol = 1, dimnames = list(x = names(res)))
      },
      sort = TRUE
    )(item1, dimension, value) %>% 
    select(-item2)
}
```

```{r}
tidy_word_vectors %>% 
  nearest_neighbors("income")
```

Output: A bar chart with top words related to the word of interest

```{r}
# Words excluded iteratively while creating the LDA model
# Custom stop words may contain the words used in the literature search query
custom_stop_words <- tibble(word = c("income",
                                     "incomes",
                                     "segregation",
                                     "neighbourhood",
                                     "neighborhood",
                                     "21",
                                     "11"))

tidy_word_vectors %>% 
  nearest_neighbors("income") %>% 
  filter(!item1 %in% custom_stop_words$word) %>% 
  slice_max(value, n = 10) %>% 
  ggplot(aes(reorder(item1, value), value)) +
  geom_col() +
  coord_flip() +
  labs(x = NULL, y = "PMI", title = "Top words related to 'income'")
```

Output: A wordcloud with top words related to the word of interest

```{r}
top_10_words <- tidy_word_vectors %>% 
  nearest_neighbors("income") %>% 
  filter(!item1 %in% custom_stop_words$word) %>%
  filter(value > 0) %>%
  mutate(freq = as.integer(value * 1000)) %>%
  slice_max(value, n = 30)
```

```{r}
#png("output/wordcloud_test.png", width=6,height=5, units='in', res=300)
set.seed(1234) # for reproducibility 
wordcloud(words = top_10_words$item1, freq = top_10_words$freq, max.words=400, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
# add a title
title(main = "Word Cloud of Keywords")
#dev.off()
```

### Topic model

We use an LDA topic model to identify dominant topics in a corpus of papers. We start by pre-processing the data and splitting it into words.

```{r tokenize}
# Words excluded iteratively while creating the LDA model
# Custom stop words may contain the words used in the literature search query
custom_stop_words <- tibble(word = c("income", "incomes", "segregation",
                                     "neighbourhood", "neighborhood"))

words <- data |>
  select(id, text, "year" = `Publication.Year`) |>
  unnest_tokens(output = word, input = text) |>  # remove punctuation, make lower case
  anti_join(tidytext::stop_words, by = "word") |>          # remove stop words
  anti_join(custom_stop_words, by = "word") |>   # remove custom stop words
  mutate(word = lemmatize_words(word)) |>        # lemmatise words
  filter(nchar(word) >= 3)                       # keep words longer than 3 letters
```

The LDA model requires a Document Term Matrix (DTM) as input.

```{r dtm}
dtm <- words |>
    count(id, word, sort = TRUE) |>
    filter(n > 5) |>  # minimum term frequency 5
    cast_dtm(id, word, n)
```

We choose the value for the `k` hyperparameter either empirically or qualitatively.

```{r k}
# Set k empirically
# # This takes around 10 minutes to run and results in a value between 25 and 30
# # which is rather difficult to interpret
# # This might be useful and easy to use if combined with hierarchical representation
# # of topics in a dendrogram - still to be done
# result <- FindTopicsNumber(
#   dtm,
#   topics = seq(from = 2, to = 30, by = 1),
#   metrics = c("CaoJuan2009",  "Deveaud2014"),
#   method = "Gibbs",
#   control = list(seed = 2023),
#   verbose = TRUE)
# k = result$topic[which.max(result$Deveaud2014 - result$CaoJuan2009)]

# Set k qualitatively, based on the researcher's understanding of the literature
k = 5
```

We fit the LDA model and extract topic distributions over terms represented by the $\beta$ statistic and the topic document distributions over topics represented by the $\theta$ statistic. For each topic, we also generate a pseudo-name from the top 5 words.

```{r lda}
# Fit LDA model
lda <- LDA(dtm, k = k, method="Gibbs",
           control = list(seed = 2023, iter = 500, verbose = 100))

# Extract beta and theta statistics from LDA model
beta <- posterior(lda)$terms
theta <- posterior(lda)$topics 

# Add pseudo-names to topics based on the top 5 words in each topic
topic_names <- c()
n_words <- 5
for (i in 1:nrow(beta)) {
  name <- paste(names(head(sort(beta[i, ], decreasing = TRUE), n_words)), collapse = " ")
  topic_names <- c(topic_names, name)
}
```

```{r}
#| lst-label: lst-ai-prompt
#| lst-cap: Promt generated from topic model parameters
keywords <- vector(length = k)

for (i in 1:k) {
    keywords[i] <- paste0("- The words for Topic ", i, " are: ", 
        topic_names[i], ".\n")
}

cat(paste0("I have ", k, " topics, each described by ", n_words, 
    " words. The keywords are as follows:\n"), keywords, 
    "How would you name these topics? Use maximum two words to name the topics and provide a one-sentence description for each.", 
    sep = "")
```

#### Topics across the corpus

Output: Bar charts showing top words in the corpus and in each topic

```{r}
terms <- as.data.frame(posterior(lda)$terms)
rownames(terms) <- topic_names

terms <- terms |>
  mutate(topic = rownames(terms)) |>
  pivot_longer(-topic,
               names_to = "term",
               values_to = "prob") |> 
  group_by(term) |>
  mutate(max_topic = topic[which.max(prob)]) |>
  filter(topic == max_topic) |>
  ungroup()

words_topics <- words |>
  left_join(terms, by = c("word" = "term"))

top_terms <- words |>
    group_by(id) |>
    count(word, sort = TRUE) |>
    ungroup() |> 
    slice_max(n, n = 20)

top_terms |>
  left_join(terms, by = c("word" = "term")) |>
  ggplot(aes(reorder(word, n), n)) +
  geom_col(aes(fill = max_topic)) +
  geom_text(aes(label = n), size = 2, hjust = 1.1) +
  coord_flip() +
  xlab("Word") +
  ylab("Frequency") +
  labs(title = paste0("Top ", 20, " most used words in the corpus of theses")) +
  theme_minimal() +
  theme(panel.grid = element_blank())
```

```{r}
# Function to approximate the distance between topics
svd_tsne <- function(x)
  tsne(svd(x)$u)

# Convert DTM into JSON required by the LDAvis package
json <- createJSON(
  phi = beta,
  theta = theta,
  doc.length = rowSums(as.matrix(dtm)),
  vocab = colnames(dtm),
  term.frequency = colSums(as.matrix(dtm)),
  mds.method = svd_tsne,
  plot.opts = list(xlab = "", ylab = "")
)

# Visualise topics model with LDAvis
LDAvis::serVis(json)
```

#### Evolution of topics

Output: Plots showing the change in relative importance of topics and top words over the years.

```{r evolution-topics}
topic_prop_per_year <- theta
colnames(topic_prop_per_year) <- topic_names

viz_df_with_year <-
  pivot_longer(
    data = data.frame(topic_prop_per_year,
                      document = factor(str_sub(
                        rownames(topic_prop_per_year), 1, 20)),
                      check.names = FALSE),
    cols = -document,
    names_to = "topic",
    values_to = "value") |> 
  left_join(mutate(data, id = as.factor(id), "year" = `Publication.Year`), by = c("document" = "id"))

# Plot topic proportions per year
viz_df_with_year |> 
  ggplot(aes(x = factor(year), y = value, fill = topic)) + 
  geom_bar(stat = "identity", position = "fill") +
  scale_fill_viridis_d(name = "Topics") +
  xlab("Publication year") +
  ylab("Proportion of topics") +
  labs(title = "Topic proportions per year")

# ggsave("output/topics-over-years.png", width = 10, height = 7)
```

```{r evolution-top-terms}
#| lst-label: lst-words-per-year
#| lst-cap: Evolution of words over the years
#| warning: false
top_terms <- words |>
  group_by(id) |>
  count(word, sort = TRUE) |>
  ungroup() |>
  slice_max(n, n = 12)

word_order <- words |>
  group_by(word, year) |>
  count(word, sort = TRUE) |>
  ungroup() |>
  arrange(year,-n) |>
  group_by(year) |>
  mutate(relative_frequency = n / sum(n) * 100) |>
  mutate(rank = round(rank(-n), 0)) |>
  ungroup() |>
  filter(word %in% top_terms$word & word != "lihtc")

ggplot(word_order,
       aes(
         x = year,
         y = relative_frequency,
         color = word,
         label = rank
       )) +
  # aes(fill = "white") +
  geom_smooth(lwd = 0.5) +
  # geom_line() +
  # geom_point(size = 8, shape = 21, fill = "white", stroke = 1.2) +
  # geom_text(ggplot2::aes(x = grad_year), color = "black") +
  ggplot2::scale_color_viridis_d() +
  ggplot2::xlab("Year") + 
  ggplot2::ylab("Frequency (%)") +
  facet_wrap(~ word) +
  theme(title = element_blank(),
        axis.text = element_text(size = 12),
        axis.title = element_text(size = 15),
        legend.position = "none")

# ggsave("output/words-over-years.png", width = 10, height = 7)
```
