---
title: "Companion code for section 2: Search strategy & selection of references"
author: "Cottineau C., Forgaci F., Janssen K., Li B., Zhang S., Zhang X."
date: "2024-03-20"
format: html
editor: visual
number-sections: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
Your_scopus_api_key <- readLines(con = "personal_scopus_api_key.md")
```

## Section 2: Search strategy & selection of references

Let us first install and load the packages we will need.
```{r}
## List pacakges used in the script
packages <- c("rscopus", "RefManageR", "tidyverse","brio",
              "stringr", "bibtex", "glue", "here", "litsearchr",
              "revtools", "remotes", "igraph", "remotes",
              "PRISMAstatement")

## Load packages and install them if needed
for (package in packages) {
  if (!require(package, character.only = TRUE)) {
    if (package == "litsearchr") remotes::install_github("elizagrames/litsearchr", ref="main")
    else install.packages(package)
  }
}

```
We also need to organize our repository 
```{r}
dir.create("data") #for raw data
dir.create("data_output") #for the unique references 
dir.create("figures")
```

### 2.1. Retrieving references from Scopus {#sec-rscopus}

Set your API key - if you do not have one, please go to the [Elsevier Developer Portal](https://dev.elsevier.com/) to apply for one with your institutional credentials.

Once you have your key (i.e. a long string of digits and characters), replace `Your_scopus_api_key` by the key value, and add quotes, for instance: options(elsevier_api_key = "my8personal4key")

```{r}
options(elsevier_api_key = Your_scopus_api_key)
```

Set your research query:
```{r}
query <- "( ( ( TITLE ( govern* OR state OR decision-making OR policy-making OR stakeholder OR participat* ) ) AND ( TITLE-ABS-KEY ( impact OR outcome OR result OR differentiation OR consequence OR change OR transformation OR role ) ) ) OR ( TITLE-ABS-KEY ( governance W/0 ( mode OR model OR process ) ) ) ) AND 
          ( TITLE-ABS-KEY ( effect OR caus* OR explain* OR influence OR affect OR mechanism OR restrict OR create OR impact OR drive OR role OR transform* OR relation* OR led OR improve OR interven* OR respon* ) ) AND 
          ( TITLE-ABS-KEY ( effect OR caus* OR explain* OR influence OR affect OR mechanism OR restrict OR create OR impact OR drive OR role OR transform* OR relation* OR led OR improve OR interven* OR respon* ) ) AND 
          ( TITLE-ABS-KEY ( urban OR neighborhood OR city OR residential OR regional OR housing ) W/0 ( development OR redevelopment OR regeneration OR restructuring OR revitalization OR construction OR governance ) ) AND 
          ( LIMIT-TO ( DOCTYPE , \"ar\" ) ) AND 
          ( LIMIT-TO ( LANGUAGE , \"English\" ) )"
```

Query scopus if you get the api key. Note that you can modify the max_count for each searching:

```{r}
if (have_api_key()) {
  res <- scopus_search(query = query, max_count = 200, count = 10, view = "COMPLETE")
  search_results <- gen_entries_to_df(res$entries)
}
```

Create an empty list to store search results
```{r}
ids <- search_results$df$pii
search_results_list <- list()
for (id in ids) {
  search_results_list[[id]] <- search_results$df
}

```

Convert the list to a data frame
```{r}
results_df <- do.call(rbind, search_results$df)
transposed_results_df <- t(results_df)
```

What does it look like?
```{r}
head(transposed_results_df)

```

Write the details to a CSV file

```{r}
write.csv(transposed_results_df, here("data", "scopus_api_results.csv"), row.names = FALSE)
```

Export a into a .bib file
```{r}
df <- read.csv(here("data", "scopus_api_results.csv"))
for (i in 1:nrow(df)) {
  df$authkeywords[i] <- paste(unlist(strsplit(df$authkeywords[i], "\\s*\\|\\s*")), collapse = ", ")
}

data <- data.frame(
  Author = df$dc.creator,
  Title = df$dc.title,
  Year = sub(".*\\s(\\d{4})$", "\\1", df$prism.coverDisplayDate),
  Journal = df$prism.publicationName,
  Volume = df$prism.volume,
  Number = df$article.number,
  Pages = df$prism.pageRange,
  DOI = df$prism.doi,
  Keyword = df$authkeywords
)
```

Create a list of BibEntry objects
```{r}
# Format the Keywords field
bib_entries <- lapply(1:nrow(data), function(i) {
  BibEntry(
    bibtype = "Article",        # Add the bibtype argument
    key = paste0(substr(data$Author[i], 1, 1), data$Year[i]),
    author = data$Author[i],    # Add author field
    title = data$Title[i],      # Add title field
    year = data$Year[i],        # Add year field
    journal = data$Journal[i],  # Add journal field
    volume = data$Volume[i],    # Add volume field
    number = data$Number[i],    # Add number field
    pages = data$Pages[i],      # Add pages field
    doi = data$DOI[i],         # Add DOI field
    url = data$Keyword[i]      # Add Keyword field (because BibEntry function does not provide keyword indicator, use url for keywords as an example.)
  )
})
```

Convert each BibEntry object to BibTeX format individually
```{r}
bib_texts <- lapply(bib_entries, toBibtex)
```

Combine the BibTeX texts into a single character vector
```{r}
bib_text <- unlist(bib_texts, use.names = FALSE)
```

Write BibTeX file
```{r}
writeLines(bib_text, here("data","scopus_references.bib"))
```

### 2.2. Combining tables, deduplicating references and summarising the results {#sec-revtools}

Although a dedicated package exist to retrieve references from the Web of Science API (`wosr`), we have not been able to make it work. Instead, we used the web interface of the (Web of Science)[https://www.webofscience.com/wos/woscc/summary/82f1ef9f-d361-4455-a556-cc37014e5f7a-de2281b0/relevance/1] (through our institutional access) to run the same query:

( ( ( TI= ( govern*  OR  state  OR  decision-making  OR  policy-making OR participat* OR stakeholder ) )  AND  ( TS= ( impact  OR  outcome  OR performance OR  result  OR  differentiation  OR  consequence  OR  change  OR  transformation OR role ) ) )  OR  ( TS= ( governance  NEAR/0  ( mode  OR  model  OR role OR process  )) ) ) AND (TS= ( effect  OR  caus*  OR  explain*  OR  influence  OR  affect  OR  mechanism  OR  restrict  OR  create  OR  impact  OR  drive  OR  role  OR  transform*  OR  relation*  OR  led  OR  improve  OR  interven*  OR  respon* ) ) AND  ( TS= (( urban  OR  neighborhood  OR  city  OR  residential  OR  regional  OR  housing )  NEAR/0 (development  OR  redevelopment OR renewal OR  regeneration  OR  restructuring  OR  revitalization  OR governance ) )  )

We then selected the articles written in English from the results and obtained 3180 results as of 11 April 2024. We downloaded the first 200 records in .bib format for this example: `wos_reference.bib`. Indeed, we want to show that this code can combine bibliographic data from different queries and or different databases.

```{r}
wos_data <-synthesisr::read_refs(here("data", "wos_reference.bib"))
scopus_data <- synthesisr::read_refs(here("data", "scopus_references.bib"))

# Remember we stored keywords into the url field, let's rename it now:
scopus_data$keywords <- scopus_data$url
```

Save variable names of dataframes is object

```{r}
unique_vars_scopus <- colnames(scopus_data)
 unique_vars_wos <- colnames(wos_data)

```


Identify which columns the scopus and wos dataframes have in common
```{r}
common_vars <- intersect(unique_vars_scopus, unique_vars_wos)
print(common_vars)

```

Identify which columns are unique for the scopus and wos references
```{r}
unique_vars_only_scopus <- setdiff(unique_vars_scopus, unique_vars_wos)
print(unique_vars_only_scopus)
unique_vars_only_wos <- setdiff(unique_vars_wos, unique_vars_scopus)
print(unique_vars_only_wos)

```

Select only the variables that appear in both dataframes and the ones you deem relevant. In our case, we select: `author`, `type`, `title`, `year`, `volume`,  `number`, `pages`, `doi`, `keywords`

```{r}
selected_vars <- c(
 # "label",
  "author",  "title", "year", "journal", 
  "volume", "number", "pages", "doi", "keywords")

scopus_selection <- scopus_data %>%
  dplyr::select(all_of(selected_vars))

wos_selection <- wos_data %>%
  dplyr::select(all_of(selected_vars))

```
Now lets check the number of variables (columns) in our dataframes

```{r}
ncol(scopus_selection)

ncol(wos_selection)
```

now lets combine the wos dataframe with the scopus dataframe

```{r}
all_references <- rbind(wos_selection, scopus_selection)

```

## Locate and extract unique references {#sec-targets}

In order to be able to identify if there are duplicates in our `all_references` dataframe and examine summary statistics of the references-dataset, we need to make sure that variables in the scopus and wos dataframes are represented in a similar way. Sometimes variables differ too much, such that comparison even after alteration becomes difficult (i.e. in our example the variable "author"). However,  for others we we can remove all punctuations and capital letters in order to make the variable structure of the wos refences and scopus refences more similar.

Lets see how we can do that.

First create function called `preprocess` which removes all capital letters and punctuations

```{r}
preprocess <- function(text) {
  text <- tolower(text) #all characters are transformed to lower-case
  text <- gsub("[[:punct:]]", "", text) #all punctuations are removed from the characters
  return(text)
}

```

Now lets apply this function on relevant variables in our `all_references` dataframe

```{r}
all_references$journal <- sapply(all_references$journal, preprocess)
all_references$title <- sapply(all_references$title, preprocess)
all_references$author <- sapply(all_references$author, preprocess)

```

Examine the scopus_selection dataframe

```{r}
view(all_references)
```

as you can see, all the capital and punctuations are removed from the assigned variables 

```{r}
check_duplicates <- revtools::find_duplicates(all_references, match_variable = "doi")
all_unique_references <- extract_unique_references(all_references, matches = check_duplicates)
```

Compare the total number of references to the total number of unique references

```{r}
#count the number of rows in the all_references dataframe
nrow(all_references)
#do the same for the references_unique dataframe
nrow(all_unique_references)


```

Something else we might want to consider, is if our literature search also identified one or more key articles that we know should be in the SLR. So, lets take a target article. In our case, the author of this specific literature review stated that the articles `Urban governance and social cohesion: Effects of urban restructuring policies in two Dutch cities` by `van Marissing et al. (2006)` and `Rethinking China’s urban governance: The role of the state in neighbourhoods, cities and regions` by `Wu and Zang (2022)` should be considered as a target references. So lets examine, if this article is present in our `all_unique_references` dataframe.  
We do this copying doi from the target references and identifying if this doi is present in the `all_unique_references` dataframe

```{r}
#create a vector with the doi's of van Marissing et al. (2006) and Wu and Zang (2022)
target_references <- c("doi.org/10.1016/j.cities.2005.11.001", "doi.org/10.1177/03091325211062171")


all_unique_references%>%
  filter(doi %in% target_references)

```
The outcome from after filtering the `all_unique_references` dataframe is 0, suggesting that none of our target references is included in our `all_unique_references` dataframe. This means that we need to modify our query so as to include them in the results. Please note that for the sake of simplicity and speed, we have restricted our search to 200 articles in each database, which is why it is most likely that we did not identify the target references in this particular example.  

### Summarise references: 

Distribution of publication years
```{r}

all_unique_references$year <- as.numeric(all_unique_references$year)

ggplot(all_unique_references) +
  geom_histogram(aes(x=year), fill = "orange")
```

Top 10 journals publishing on topic:

```{r}
all_unique_references$journal <- as.factor(all_unique_references$journal)
head(summary(all_unique_references$journal),10)

```


Export combined set of unique references into a .bib file:
```{r}
unique_data <- data.frame(
    Author = all_unique_references$author,
  Title = all_unique_references$title,
  Year = all_unique_references$year,
  Journal = ifelse(is.na(all_unique_references$journal), 
                     "Unknown",all_unique_references$journal),
  Volume = all_unique_references$volume,
  Number = all_unique_references$number,
  Pages = all_unique_references$pages,
  DOI = all_unique_references$doi,
  Keywords = all_unique_references$keywords
)

# Format the Keywords field
unique_bib_entries <- lapply(1:nrow(unique_data)
                             , function(i) {
  BibEntry(
    bibtype = "Article",        # Add the bibtype argument
    key = i,
    author = unique_data$Author[i],    # Add author field
    title = unique_data$Title[i],      # Add title field
    year = unique_data$Year[i],        # Add year field
    journal = unique_data$Journal[i],  # Add journal field
    volume = unique_data$Volume[i],    # Add volume field
    number = unique_data$Number[i],    # Add number field
    pages = unique_data$Pages[i],      # Add pages field
    doi = unique_data$DOI[i],         # Add DOI field
    url = unique_data$Keywords[i]
    )
})

unique_bib_texts <- lapply(unique_bib_entries, toBibtex)
unique_bib_text <- unlist(unique_bib_texts, use.names = FALSE)
writeLines(unique_bib_text, here("data_output","all_unique_references.bib"))

```

## 2.3. Suggesting new keywords {#sec-litsearchr}


Import .bib or .ris database
```{r}

refs <- litsearchr::import_results(here("data_output"))

```

Identify frequent terms
```{r}
raked_terms <- extract_terms(text = refs,
                             method = "fakerake",
                             min_freq=2,
                             min_n=2)
```

Identify frequent keywords tagged by authors
```{r}

keywords <- extract_terms(text = refs,
                          method = "tagged",
                          keywords = refs$url, # remember: we stored keywords into the url field...
                          ngrams=T,
                          min_n=2)
```

Create document-feature matrix
```{r}
dfm <- create_dfm(elements = refs$title,
                  features = c(raked_terms, keywords))

```

Create a semantic network
```{r}
net <- create_network(search_dfm = dfm,
                      min_studies = 3,
                      min_occ = 3)
hist(igraph::strength(net),
     main="Histogram of node strengths",
     xlab="Node strength")


cutoffs_cumulative <- find_cutoff(net, method = "cumulative")

reduced_graph <- reduce_graph(net, cutoff_strength = cutoffs_cumulative)

plot(reduced_graph)

```


Identify the main keywords
```{r}
search_terms <- litsearchr::get_keywords(reduced_graph)
head(sort(search_terms), 20)
```

Identify isolated components of graph to suggest new keywords
```{r}
components(reduced_graph)
grouped <- split(names(V(reduced_graph)), components(reduced_graph)$membership)
```

Write a new query based on additional keywords
```{r}
litsearchr::write_search(grouped,
                         API_key = NULL,
                         languages = "English",
                         exactphrase = FALSE,
                         stemming = TRUE,
                         directory = "./",
                         writesearch = FALSE,
                         verbose = TRUE,
                         closure = "left")
```



## 2.4. Drawing the PRISMA figure {#sec-prisma}

Call the prisma() function to generate the PRISMA flowchart and replace the values with the actual counts from your study.
```{r}
# 
prismaplot <- prisma(
  found = nrow(all_references), # Total number of references found
  found_other = 0,               # Number of additional references found through other sources (if any)
  no_dupes = nrow(all_unique_references), # Number of unique references after removing duplicates
  screened = nrow(all_unique_references),  # Number of references screened
  screen_exclusions = 0,         # Number of references excluded during screening
  full_text = nrow(all_unique_references), # Number of references obtained in full text
  full_text_exclusions = 0,      # Number of references excluded during full-text assessment
  qualitative = nrow(all_unique_references), # Number of studies included in qualitative synthesis
  quantitative = nrow(all_unique_references), # Number of studies included in quantitative synthesis
  width = 800, height = 800  ,    # Specify width and height for the generated PRISMA flowchart
  font_size = 10
)

```


Save the Prisma diagram as an object and export it as .png file
```{r}
png(here("figures","prisma_flowchart.png"), width = 800, height = 800)
prismaplot
dev.off()
```


